# HCAI Project topic research

started this on 24/04/2025

## Possible topics of the project

- How non-technical people write prompts
- Main challenges faced by non-techincal people when writing prompts
- Why are some prompts more effective than others

## Chosen Domain - Evaluation of potential threats for users in HCAI systems

Big idea question: "What are some possible ways AI could be abused or exploited by bad actors"

Possible sub-topics for discussion:

- prompt injection
- contextual manipulation
- jailbreaking

### prompt injection as a threat vectors in LLM supplimented applications

- possible consequences of prompt injection #human-aspect
    - how humans may be affected (direct & in-direct) #human-aspect
    - other widespread affects (misinformation, economic loss, etc.) #human-aspect
- hiding prompts in places humans can't read #HCAI-aspect #realiability
    - like in hidden text
    - like in images
    - like in encoded data
    - like in program code
- possible ways to catch prompt injection #HCAI-aspect #trustworthiness
- possible ways to prevent prompt injection #HCAI-aspect #reliability

### Possible papers to read for the above domain

- [Prompt Injection attack against LLM-integrated Applications](https://arxiv.org/pdf/2306.05499)
- [Not what youâ€™ve signed up for: Compromising Real-World
LLM-Integrated Applications with Indirect Prompt Injection](https://arxiv.org/pdf/2302.12173)
- [EVALUATING THE SUSCEPTIBILITY OF PRE-TRAINED LANGUAGE MODELS VIA HANDCRAFTED ADVERSARIAL EXAMPLES](https://arxiv.org/pdf/2209.02128)

